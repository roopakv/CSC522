\documentclass[fontsize=10pt,DIV=14]{scrartcl}

\usepackage{enumitem}
	\setenumerate{listparindent=\parindent}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{hyperref}
\usepackage{float}
\usepackage{tabularx}
\usepackage{listings}

\newcommand{\code}{\texttt}

\begin{document}

	\title{CSC 522 : Automated Learning and Data Analysis}
	\subtitle{Homework 5}
	\author{Roopak Venkatakrishnan - rvenkat7@ncsu.edu}
	\maketitle

	\section{Question 1 [57 Points (25 + 32)] - Regression}

	In this problem we will investigate various methods for ﬁtting a linear model for regression. Download the regprob.zip ﬁle from the course website.

	\begin{enumerate}
		\item
		Given a set of n real-valued responses $y_{i}$ and a set of p predictors, we might try to model $y_{i}$ as a linear combination of the p predictors. The form of this type of linear model is:
		\begin{equation*}
			y_{i} = \beta_{0} +\sum_{j=1}^{p} + \beta_{j} \times x_{ij}
		\end{equation*}

		where $y_{i}$ is the the value of the response for the $i^{th}$ observation, $x_{ij}$ is the value for the $j^{th}$ predictor for observation i, and $\beta_{0}$ is the intercept. To ﬁnd ’good’ values for all of the $\beta$s, one approach is to minimize the sum of squared errors (SSE), shown below:

		\begin{equation*}
			SSE =\sum_{i=1}^{n} (y_{i} - \beta_{0} - \sum_{j=1}^{p} \beta_{j} \times x_{ij})
		\end{equation*}

		This approach is known as regression via ordinary least squares (OLS). Representing this model in matrix notation, the model can be written in an equivalent form as $Y = X\beta$. Now Y is an $n \times 1$ column vector containing the response variable, X is an $n \times (p + 1)$ matrix that contains the $p$ predictors for all $n$ observations as well as a column of all 1s to represent the intercept, and $\beta$ is a $p + 1$ vector. With some matrix calculus it can be shown the value of $\beta$ that minimizes the SSE is given by:

		\begin{equation*}
			\hat{\beta}_{OLS} = (X^{T} X)^{-1} X^{T}Y
		\end{equation*}

		where $T$ indicates a matrix transpose. This formula will give a $(p + 1)$ vector containing the estimated regression coeﬃcients.

		Complete the following tasks:
		\begin{itemize}
			\item
			Load \emph{train.csv}

\begin{verbatim}
> train <- read.csv(file.choose())
\end{verbatim}
			
			\item
			Compute the OLS estimates using the data in train.csv. Do not use a package to do this, instead compute it directly from the formula given above. There are 10 predictors in the ﬁle, so your solution should contain 11 estimated regression coeffcients (1 for each predictor plus 1 for the intercept, 11 numbers in total).

			\begin{lstlisting}[language=R,frame=single]
> library(caret)
Loading required package: cluster
Loading required package: foreach
foreach: simple, scalable parallel programming from Revolution Analytics
Use Revolution R for scalability, fault tolerance and more.
http://www.revolutionanalytics.com
Loading required package: lattice
Loading required package: plyr
Loading required package: reshape2
> x_data <- train[2:11]
> y_data <- train[1]
> X0 <- rep(1,100)
> x_data <- cbind(X0,x_data)
> xt <- t(x_data)
> xtx <- as.matrix(xt) %*% t(xt)
> xty <- as.matrix(xt) %*% as.matrix(y_data)
> beta <- solve(xtx) %*% xty
> beta
                Y
X0   2.0011897376
X1   1.4866088726
X2  -1.9616801211
X3   3.0082822263
X4   1.7619676828
X5  -0.4978060382
X6  -0.0319859478
X7   0.0120974698
X8  -0.0006889951
X9  -0.0060084271
X10  0.0112536257
			\end{lstlisting}
			\textbf{Note:} In the above case $X0$ is the coeffiecient of $\beta$ for the intercept
			\item
			Estimate the mean squared error on an unseen test set by performing 5-fold crossvalidation. Recall the MSE for a set of $y$ observations and $\hat{y}$ predictions is deﬁned as
			\begin{equation*}
				MSE = \frac{1}{N} \sum_{i=1}^{n} (y_{i} - \hat{y_{i}})^{2}
			\end{equation*}

			\begin{lstlisting}[language=R,frame=single]
> folds2 <-createFolds(train[["Y"]],k=5,list=FALSE)
> mse <- rep(0,5)
> for(i in 1:5) {  
+  
+   fold.rows <- which(folds2 == i)
+   cv.train <- train[-fold.rows,]
+   
+   cv.test <- train[fold.rows,]
+   
+   x_train <- cv.train[2:11]
+   y_train <- cv.train[1]
+   X0 <- rep(1,80)
+   x_train <- cbind(X0,x_train)
+   xt <- t(x_train)
+   xtx <- as.matrix(xt) %*% t(xt)
+   xty <- as.matrix(xt) %*% as.matrix(y_train)
+   beta <- solve(xtx) %*% xty
+   
+   x_test <- cv.test[2:11]
+   y_act <- cv.test[1]
+   xpred <-  mapply("*",t(beta)[2:11],x_test)
+   xpred <- cbind(t(beta)[1],xpred)
+   y_pred <- rowSums(xpred)
+   ydiff <- cbind(y_act,y_pred)
+   ydiff$diff <- ydiff$Y - ydiff$y_pred
+   yd_sq <- ydiff$diff^2
+   mse[i] <- sum(yd_sq)/20
+ }
> mse
[1] 0.03354157 0.04317550 0.06382128 0.03696788 0.04751039
> mean(mse)
[1] 0.04500332
			\end{lstlisting}

			We get the Mean MSE to be $0.04500332$.
		\end{itemize}

		\item
		The term `linear model' indicates that a model is linear with respect to $\beta$. However, we can model higher order polynomial terms by explicity computing them, including them in the $X$ matrix, and then ﬁt a linear model to this matrix. Perform the following tasks:

		\begin{itemize}
			\item
			Load \emph{polynomial.train.csv}
			\begin{verbatim}
> data <- read.csv(file.choose())
			\end{verbatim}

			\item
			Plot $Y$ as a function of $X$
\begin{verbatim}
> plot(data[["X"]],data[["Y"]],xlab="X data",ylab="Y data")
\end{verbatim}
			
			\begin{figure}[H]
				\begin{center}
					\includegraphics[width=\textwidth]{resources/q1_2_img1.png}
					\caption{Plot with points}
				\end{center}
			\end{figure}
			\item
			Create a new $X$ matrix that includes a column of 1s for an intercept, a column for the original $X$ values, and a column of polynomials for each $X^{i}$ for $i \in {2,3,4,5}$. This will create a matrix with dimensions $300 \times 6$.
			\begin{verbatim}
> x_data <- cbind(rep(1,300),data["X"],data["X"]^2,data["X"]^3,data["X"]^4,data["X"]^5)
> names(x_data) <- c("X0","X1","X2","X3","X4","X5")
			\end{verbatim}

			\item
			Find the OLS solution to this using $(X^{T}X)^{-1} X^{T}Y$.

\begin{lstlisting}[language=R,frame=single]
> xt <- t(x_data)
> xtx <- as.matrix(xt) %*% t(xt)
> xty <- as.matrix(xt) %*% as.matrix(data["Y"])
> beta <- solve(xtx) %*% xty
> beta
               Y
X0  2.0142724145
X1  0.9522479087
X2  0.5014464975
X3 -0.2219555459
X4  0.0001422326
X5 -0.0031247916
\end{lstlisting}
		\textbf{Note:} Here $X0$ is the intercept while $X1,X2,X3,X4,X5$ denote powers of $X$ etc.

		\item
		Overlay the fitted values (i.e. $X\hat{\beta}_{OLS}$) as a line on the plot of $Y$ vs. $X$.
		\begin{lstlisting}[language=R,frame=single]
> xpred <-  mapply("*",t(beta),x_data)
> y_pred <- rowSums(xpred)
> pred<- cbind(y_pred,x_data[2])
> pred_out <- arrange(pred,X1)
> plot(data[["X"]],data[["Y"]],xlab="X data",ylab="Y data")
> lines(pred_out$X1,pred_out$y_pred,col="red")
		\end{lstlisting}
		\begin{figure}[H]
				\begin{center}
					\includegraphics[width=\textwidth]{resources/q1_2_img2.png}
					\caption{Plot with points and overlaid line}
				\end{center}
			\end{figure}
		\end{itemize}
	\end{enumerate}

	\section{Question 2 [30 Points] - Artiﬁcial Neural Network}

	Consider the dataset Image Segmentation Data Set from the UCI repository \url{http://archive.ics.uci.edu/ml/datasets/Image+Segmentation}. The dataset consists of 19 precomputed attributes of 7 outdoor images, or 7 classes. You are provided with a training set and a test set. 

	Unlike the problems you are seen in the past, which have all been binary classiﬁcation, this problem has seven classes. With Artiﬁcial Neural Network, there are at least 2 ways to construct a multi-class classiﬁer:

	\begin{enumerate}
		\item
		Direct approach, where there are 7 output nodes to a neural network

		\item
		One-vs-All classiﬁcation, where you build one binary classiﬁer per class for each of the 7 classes. When predicting the correct class for a given instance in the test data, we choose the classiﬁer that has the highest conﬁdence.

	\end{enumerate}

	Your task is to build the best possible 7-output ANN and the best possible One-vs-All classiﬁer. You must submit the following:

	\begin{itemize}
		\item
		A description of how you built the classiﬁers including the parameters you chose and the reason behind such a choice. The parameters include epoch, momentum, learning rate, number of hidden nodes and any other parameter you think might help.

		I used the multiLayer Perceptronin Weka for the Direct Approach. When trying to find values for the number of hidden nodes, I did a bit fo searching to see what would be th optimum number of nodes to choose and came across this article at \url{ftp://ftp.sas.com/pub/neural/FAQ3.html#A_hu}. After reading it I tried their suggestion of choosing the number of nodes as $( inputs + outputs) * \frac{2}{3}$. Thus the number of hidden nodes was chosen to be 17.

		\item
		A descriptive comparison in performance between the 7-output ANN and the One-vs-All ANN - compare the 2 models based on their predictive performance on the given test data, training time, and your judgment of which approach is better for this problem.

		\item
		Any code you have written (using Matlab, R, Weka’s Java API)
	\end{itemize}

	\section{Question 3 [10 Points (6 + 4)] - Multi-Class Classification}


	An electronic nose is a device that can ``sniff'' gases at various locations. One way to construct the devise is using an array of $N$ semiconductors, each of which will have a different voltage response when in contact with certain gases. Each semiconductor responds to at least one gas (i.e., more than one gas). Let us assume that there are 3 gases A, B and C. Some locations can have either one of the gases or a mixture of gases. Thus, possible class labels are: A, B, C, AB, AC, BC, ABC.

	\begin{enumerate}
		\item
		If you are allowed to use only an Artiﬁcial Neural Network, which of the following con-ﬁgurations are possible? State why or why not.
		\begin{itemize}
			\item
			1 network with 7 output nodes.

			This would be the simple choice. The output nodes would be $A, B, C, AB, AC, BC, ABC$. Thus this configuration is possible. We have to keep in mind here the fact that if we decide to say a gas is present if the confidence of the presence of that gas is above say 60\%. Then in that case at a place X if we see that gas A is present with 90\% confidence and both B and C are present with say 62\% confidence. Then though we are almost sure that A is present 

			\item
			One-vs-All

			This is again possible if we are trying to find presence of one vs others etc. So it could be $A$ present vs $A$ not present or maybe $AC$ present vs $AC$ not present

			\item
			1 network with only 3 output nodes

			This is possible if the required output can be defined or grouped into 3 groups. One example would be presence of \{one gas,two gases, 3 gases\} i.e $\{\{A,B,C\},\{AB,AC,AC\},\{ABC\}\}$
		\end{itemize}

		\item
		Irrespective of your answer for the previous part, for each of the above conﬁgurations, comment on the complexity of the network. Comment on how you would choose the number of hidden nodes, training time and number of epochs for each of the networks.

	\end{enumerate}

	\section{Question 4 [8 Points] - Hyperplanes for Classification}

	Consider $N$ points in a D-dimensional space, some of which are positive and some of which are negative. We all know that for $N = 2$ points in $d = 1$ dimensions, a line can separate positive and negative examples. Based on this, state whether a similar linear separator is possible for each of the following cases. If a linear separator is not possible, give an example and state conditions that must be satisﬁed for the existence of a linear classiﬁer. The correct answer to this question considers all possible arrangements of the N points in the D-dimensional space. If a linear separator is not possible for even one such arrangement, your answer should state that case as an example for failure and state the conditions when a linear separator is possible.
	\begin{itemize}
		\item
		N=3, D=2

		\item
		N=4, D=2

		\item
		N=4, D=3

		\item
		N=5, D=3
	\end{itemize}
\end{document}